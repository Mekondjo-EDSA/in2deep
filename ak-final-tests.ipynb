{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# norfair dependencies\n%cd /kaggle/input/norfair031py3/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir /kaggle/working/tmp\n!cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n%cd /kaggle/working/tmp/filterpy-1.4.5/\n!pip install .\n!rm -rf /kaggle/working/tmp\n\n# norfair\n%cd /kaggle/input/norfair031py3/\n!pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index\n%cd ..","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:26:39.703099Z","iopub.execute_input":"2022-02-08T16:26:39.703971Z","iopub.status.idle":"2022-02-08T16:27:52.835988Z","shell.execute_reply.started":"2022-02-08T16:26:39.703917Z","shell.execute_reply":"2022-02-08T16:27:52.835172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport shutil\nimport sys\nimport time\nsys.path.append('../input/tensorflow-great-barrier-reef')\nimport torch\nfrom PIL import Image, ImageDraw\nimport ast\nimport albumentations as albu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-08T16:27:52.838332Z","iopub.execute_input":"2022-02-08T16:27:52.838617Z","iopub.status.idle":"2022-02-08T16:27:52.847061Z","shell.execute_reply.started":"2022-02-08T16:27:52.83858Z","shell.execute_reply":"2022-02-08T16:27:52.846223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:27:52.849114Z","iopub.execute_input":"2022-02-08T16:27:52.849929Z","iopub.status.idle":"2022-02-08T16:27:52.857002Z","shell.execute_reply.started":"2022-02-08T16:27:52.849871Z","shell.execute_reply":"2022-02-08T16:27:52.856291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_DIM = (1280,720)\n\nEXCLUDE_MARGIN = 0.05 #we will not be adding boxes in 5% of the image width or height","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:27:52.859071Z","iopub.execute_input":"2022-02-08T16:27:52.859474Z","iopub.status.idle":"2022-02-08T16:27:52.865294Z","shell.execute_reply.started":"2022-02-08T16:27:52.859438Z","shell.execute_reply":"2022-02-08T16:27:52.86454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef read_data():\n    df_train = pd.read_csv('../input/tensorflow-great-barrier-reef/train.csv')\n    df_train['img_path'] = os.path.join('../input/tensorflow-great-barrier-reef/train_images')+\"/video_\"+df_train.video_id.astype(str)+\"/\"+df_train.video_frame.astype(str)+\".jpg\"\n    df_train['annotations'] = df_train['annotations'].apply(lambda x: ast.literal_eval(x))\n    df_train['bboxes'] = df_train['annotations'].apply(lambda x: get_bbox(x))\n    df_train['Number_bbox'] = df_train['annotations'].apply(lambda x:len(x)) \n    return df_train","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:27:52.867941Z","iopub.execute_input":"2022-02-08T16:27:52.868585Z","iopub.status.idle":"2022-02-08T16:27:52.876156Z","shell.execute_reply.started":"2022-02-08T16:27:52.868548Z","shell.execute_reply":"2022-02-08T16:27:52.875384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = read_data()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:27:52.877639Z","iopub.execute_input":"2022-02-08T16:27:52.878203Z","iopub.status.idle":"2022-02-08T16:27:53.488579Z","shell.execute_reply.started":"2022-02-08T16:27:52.878167Z","shell.execute_reply":"2022-02-08T16:27:53.48788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train[df_train.annotations.str.len() > 12].head(1)\nprint(df_train['annotations'].iloc[[9325]].values)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:27:53.49008Z","iopub.execute_input":"2022-02-08T16:27:53.490354Z","iopub.status.idle":"2022-02-08T16:27:53.495848Z","shell.execute_reply.started":"2022-02-08T16:27:53.490319Z","shell.execute_reply":"2022-02-08T16:27:53.495122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shift next annotations to previous frame\ndf_shift = df_train.shift(-1).rename(columns={'annotations':'annotations_n1',\n                                             'Number_bbox':'Number_bbox_n1',\n                                             'img_path':'img_path_n1'})\ndf_lagged = pd.concat([df_train, df_shift], axis=1)\n\n#identify frames that have less annotations then the next one\n#these are the candidates for adding earlier annotations\ndf_first_frames = df_lagged[df_lagged.Number_bbox < df_lagged.Number_bbox_n1]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:27:53.497352Z","iopub.execute_input":"2022-02-08T16:27:53.497839Z","iopub.status.idle":"2022-02-08T16:27:53.531725Z","shell.execute_reply.started":"2022-02-08T16:27:53.497805Z","shell.execute_reply":"2022-02-08T16:27:53.530919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_first_frames.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:27:53.532866Z","iopub.execute_input":"2022-02-08T16:27:53.533122Z","iopub.status.idle":"2022-02-08T16:27:53.566024Z","shell.execute_reply.started":"2022-02-08T16:27:53.53309Z","shell.execute_reply":"2022-02-08T16:27:53.565143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def intersects(rectangle_a, rectangle_b):\n    '''Checks for intersection of two rectangles specified as [(x1,y1),(x2,y2)]'''\n    if(rectangle_a[1][0]<rectangle_b[0][0] or rectangle_a[1][1]<rectangle_b[0][1]):\n        return False\n    elif(rectangle_a[0][0]>rectangle_b[1][0] or rectangle_a[0][1]>rectangle_b[1][1]):\n        return False\n    else:\n        return True\n        \ndef new_bboxes(prev_bboxes, next_bboxes):\n    '''Returns the bounding boxes that are deemed new in the next frame by checking \n    the centers of the bounding box in the next frame are not contained in\n    one of the previous frame bounding boxes.'''\n    new_bbs =[]\n    delta_xs = [0]\n    delta_ys = [0]\n    delta_ws = [0]\n    delta_hs = [0]\n    for bb in next_bboxes:\n        found = False\n        for prev_bb in prev_bboxes:\n            if intersects([(bb['x'],bb['y']),(bb['x'] + bb['width'],bb['y'] + bb['height'])],\n                         [(prev_bb['x'], prev_bb['y']), (prev_bb['x'] + prev_bb['width'], \n                                                         prev_bb['y'] + prev_bb['height'])]\n                         ):\n                delta_xs.append(bb['x']-prev_bb['x'])\n                delta_ys.append(bb['y']-prev_bb['y'])\n                delta_ws.append(bb['width']-prev_bb['width'])\n                delta_hs.append(bb['height']-prev_bb['height'])\n                found = True\n                break\n        if found == False:\n            #exclude margins\n            if (bb['x'] > IMAGE_DIM[0]*EXCLUDE_MARGIN) & \\\n            (bb['x'] < (IMAGE_DIM[0]-IMAGE_DIM[0]*EXCLUDE_MARGIN)) & \\\n            (bb['y'] > IMAGE_DIM[1]*EXCLUDE_MARGIN) & \\\n            (bb['y'] < (IMAGE_DIM[1]-IMAGE_DIM[1]*EXCLUDE_MARGIN)):\n                new_bb = {'x': bb['x'], 'y': bb['y'], 'width':bb['width'], 'height':bb['height']}\n                new_bbs.append(new_bb)\n                \n    #adjust bounding boxes for avergage drift\n    for b in new_bbs:        \n        delta_x_avg = sum(delta_xs)/len(delta_xs)\n        delta_y_avg = sum(delta_ys)/len(delta_ys)\n        delta_w_avg = sum(delta_ws)/len(delta_ws)\n        delta_h_avg = sum(delta_hs)/len(delta_hs)\n        b['x'] = b['x'] + delta_x_avg\n        b['y'] = b['y'] + delta_y_avg\n               \n    return new_bbs","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:27:53.567466Z","iopub.execute_input":"2022-02-08T16:27:53.567886Z","iopub.status.idle":"2022-02-08T16:27:53.584993Z","shell.execute_reply.started":"2022-02-08T16:27:53.567848Z","shell.execute_reply":"2022-02-08T16:27:53.584212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_first_frames['new_annotations'] = df_first_frames.apply(lambda x: \n                                                            new_bboxes(x['annotations'],\n                                                                      x['annotations_n1']),\n                                                          axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:27:53.58643Z","iopub.execute_input":"2022-02-08T16:27:53.586718Z","iopub.status.idle":"2022-02-08T16:27:53.613629Z","shell.execute_reply.started":"2022-02-08T16:27:53.586679Z","shell.execute_reply":"2022-02-08T16:27:53.61271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def viz_new_boxes(prev_path, next_path, prev_annots, next_annots, new_annots):    \n    #previuos frame\n    print(prev_path)\n    img = Image.open(prev_path)\n    \n    for box in prev_annots:\n        shape = [box['x'], box['y'], box['x']+box['width'], box['y']+box['height']]\n        ImageDraw.Draw(img).rectangle(shape, outline =\"red\", width=3)\n\n    for box in new_annots:\n        shape = [box['x'], box['y'], box['x']+box['width'], box['y']+box['height']]\n        ImageDraw.Draw(img).rectangle(shape, outline =\"yellow\", width=3)\n\n    display(img)    \n    \n    #next frame\n    print(next_path)\n    img = Image.open(next_path)\n    \n    for box in next_annots:\n        shape = [box['x'], box['y'], box['x']+box['width'], box['y']+box['height']]\n        ImageDraw.Draw(img).rectangle(shape, outline =\"red\", width=3)\n\n    #for box in new_annots:\n    #    shape = [box['x'], box['y'], box['x']+box['width'], box['y']+box['height']]\n    #    ImageDraw.Draw(img).rectangle(shape, outline =\"orange\", width=3)\n        \n    display(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:27:53.616643Z","iopub.execute_input":"2022-02-08T16:27:53.617299Z","iopub.status.idle":"2022-02-08T16:27:53.627337Z","shell.execute_reply.started":"2022-02-08T16:27:53.617271Z","shell.execute_reply":"2022-02-08T16:27:53.626645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in df_first_frames.sample(10, random_state=12).iterrows():\n    viz_new_boxes(row.img_path,\n                  row.img_path_n1,\n                  row.annotations,\n                  row.annotations_n1,\n                  row.new_annotations)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:27:53.628544Z","iopub.execute_input":"2022-02-08T16:27:53.628887Z","iopub.status.idle":"2022-02-08T16:28:01.461276Z","shell.execute_reply.started":"2022-02-08T16:27:53.628847Z","shell.execute_reply":"2022-02-08T16:28:01.459008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_first_frames.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:01.465163Z","iopub.execute_input":"2022-02-08T16:28:01.465912Z","iopub.status.idle":"2022-02-08T16:28:01.500376Z","shell.execute_reply.started":"2022-02-08T16:28:01.46586Z","shell.execute_reply":"2022-02-08T16:28:01.499696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_first_frames_strip = df_first_frames[['new_annotations']]\ndf_first_frames_strip.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:01.501703Z","iopub.execute_input":"2022-02-08T16:28:01.502382Z","iopub.status.idle":"2022-02-08T16:28:01.518128Z","shell.execute_reply.started":"2022-02-08T16:28:01.502348Z","shell.execute_reply":"2022-02-08T16:28:01.517456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:01.519516Z","iopub.execute_input":"2022-02-08T16:28:01.52001Z","iopub.status.idle":"2022-02-08T16:28:01.52538Z","shell.execute_reply.started":"2022-02-08T16:28:01.519972Z","shell.execute_reply":"2022-02-08T16:28:01.524542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_new = df_train.join(df_first_frames_strip)\ndf_train_new.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:01.526756Z","iopub.execute_input":"2022-02-08T16:28:01.52739Z","iopub.status.idle":"2022-02-08T16:28:01.548163Z","shell.execute_reply.started":"2022-02-08T16:28:01.527353Z","shell.execute_reply":"2022-02-08T16:28:01.547537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train_new['new_annotations'].fillna([], inplace = True)\n#df_train_new['new_annotations'].apply(lambda x: [np.nan] if pd.isnull(x) else x)\ndf_train_new['new_annotations'].loc[df_train_new['new_annotations'].isnull()] = df_train_new['new_annotations'].loc[df_train_new['new_annotations'].isnull()].apply(lambda x: []) ","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:01.549388Z","iopub.execute_input":"2022-02-08T16:28:01.54963Z","iopub.status.idle":"2022-02-08T16:28:01.568791Z","shell.execute_reply.started":"2022-02-08T16:28:01.549599Z","shell.execute_reply":"2022-02-08T16:28:01.567422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_new['merge_annotations'] = df_train_new.apply(lambda x: (x['annotations'] + x['new_annotations']),axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:01.570042Z","iopub.execute_input":"2022-02-08T16:28:01.570299Z","iopub.status.idle":"2022-02-08T16:28:02.179306Z","shell.execute_reply.started":"2022-02-08T16:28:01.570266Z","shell.execute_reply":"2022-02-08T16:28:02.178561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_new.head(2)\n#len(df_train_new)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:02.180779Z","iopub.execute_input":"2022-02-08T16:28:02.181301Z","iopub.status.idle":"2022-02-08T16:28:02.196194Z","shell.execute_reply.started":"2022-02-08T16:28:02.181265Z","shell.execute_reply":"2022-02-08T16:28:02.19546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(df_train_new['annotations'].apply(lambda x:len(x))).sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:02.197607Z","iopub.execute_input":"2022-02-08T16:28:02.197866Z","iopub.status.idle":"2022-02-08T16:28:02.221265Z","shell.execute_reply.started":"2022-02-08T16:28:02.197829Z","shell.execute_reply":"2022-02-08T16:28:02.220539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(df_train_new['new_annotations'].apply(lambda x:len(x))).sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:02.222872Z","iopub.execute_input":"2022-02-08T16:28:02.223251Z","iopub.status.idle":"2022-02-08T16:28:02.246326Z","shell.execute_reply.started":"2022-02-08T16:28:02.223214Z","shell.execute_reply":"2022-02-08T16:28:02.2455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(df_train_new['merge_annotations'].apply(lambda x:len(x))).sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:02.247811Z","iopub.execute_input":"2022-02-08T16:28:02.248103Z","iopub.status.idle":"2022-02-08T16:28:02.267772Z","shell.execute_reply.started":"2022-02-08T16:28:02.248068Z","shell.execute_reply":"2022-02-08T16:28:02.266923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_new['New_number_bbox'] = df_train_new['merge_annotations'].apply(lambda x:len(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:02.269647Z","iopub.execute_input":"2022-02-08T16:28:02.270095Z","iopub.status.idle":"2022-02-08T16:28:02.290856Z","shell.execute_reply.started":"2022-02-08T16:28:02.270061Z","shell.execute_reply":"2022-02-08T16:28:02.290222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prev_box_count = df_train_new['Number_bbox'].sum()\ncurr_box_count = df_train_new['New_number_bbox'].sum()\nprev_frames_with_box_count = df_train_new[df_train_new.Number_bbox >0]['video_id'].count()\ncurr_frames_with_box_count = df_train_new[df_train_new.New_number_bbox >0]['video_id'].count()\nprint(\"Previous number of bounding boxes: \", prev_box_count)\nprint(\"New number of boxes: \", curr_box_count)\nprint(\"Number of boxes increase: \", curr_box_count-prev_box_count)\nprint(\"Previous number of frames with boxes: \", prev_frames_with_box_count)\nprint(\"New number of frames with boxes: \", curr_frames_with_box_count)\nprint(\"Number of frames with boxes increase: \", curr_frames_with_box_count-prev_frames_with_box_count)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:02.2934Z","iopub.execute_input":"2022-02-08T16:28:02.293591Z","iopub.status.idle":"2022-02-08T16:28:02.312156Z","shell.execute_reply.started":"2022-02-08T16:28:02.293569Z","shell.execute_reply":"2022-02-08T16:28:02.311423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_new.tail(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:02.314181Z","iopub.execute_input":"2022-02-08T16:28:02.314443Z","iopub.status.idle":"2022-02-08T16:28:02.340289Z","shell.execute_reply.started":"2022-02-08T16:28:02.314407Z","shell.execute_reply":"2022-02-08T16:28:02.339478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['annotations'] = df_train_new['merge_annotations']","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:02.342136Z","iopub.execute_input":"2022-02-08T16:28:02.34263Z","iopub.status.idle":"2022-02-08T16:28:02.349191Z","shell.execute_reply.started":"2022-02-08T16:28:02.342591Z","shell.execute_reply":"2022-02-08T16:28:02.348411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_path(row):\n    row['image_path'] = f'{ROOT_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n    return row","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:28:02.350777Z","iopub.execute_input":"2022-02-08T16:28:02.351117Z","iopub.status.idle":"2022-02-08T16:28:02.356577Z","shell.execute_reply.started":"2022-02-08T16:28:02.351083Z","shell.execute_reply":"2022-02-08T16:28:02.355858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Data\ndf = df_train.copy()\ndf['image_path'] = f'/kaggle/input/tensorflow-great-barrier-reef/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\n#df['annotations'] = df['annotations'].progress_apply(eval)\ndisplay(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:32:59.127626Z","iopub.execute_input":"2022-02-08T16:32:59.128038Z","iopub.status.idle":"2022-02-08T16:32:59.215214Z","shell.execute_reply.started":"2022-02-08T16:32:59.127998Z","shell.execute_reply":"2022-02-08T16:32:59.214488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FDA_reference = df[df['annotations']!='[]']","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:32:59.566952Z","iopub.execute_input":"2022-02-08T16:32:59.567756Z","iopub.status.idle":"2022-02-08T16:32:59.582687Z","shell.execute_reply.started":"2022-02-08T16:32:59.567707Z","shell.execute_reply":"2022-02-08T16:32:59.581742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FDA_trans = albu.FDA(FDA_reference['image_path'].values)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:32:59.869292Z","iopub.execute_input":"2022-02-08T16:32:59.871459Z","iopub.status.idle":"2022-02-08T16:32:59.875696Z","shell.execute_reply.started":"2022-02-08T16:32:59.87142Z","shell.execute_reply":"2022-02-08T16:32:59.874989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of BBoxes","metadata":{}},{"cell_type":"code","source":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts()/len(df)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:01.286982Z","iopub.execute_input":"2022-02-08T16:33:01.287558Z","iopub.status.idle":"2022-02-08T16:33:01.393937Z","shell.execute_reply.started":"2022-02-08T16:33:01.287518Z","shell.execute_reply":"2022-02-08T16:33:01.393081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🔨 Helper","metadata":{}},{"cell_type":"code","source":"def voc2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    voc  => [x1, y1, x2, y1]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n    \n    w = bboxes[..., 2] - bboxes[..., 0]\n    h = bboxes[..., 3] - bboxes[..., 1]\n    \n    bboxes[..., 0] = bboxes[..., 0] + w/2\n    bboxes[..., 1] = bboxes[..., 1] + h/2\n    bboxes[..., 2] = w\n    bboxes[..., 3] = h\n    \n    return bboxes\n\ndef yolo2voc(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n    \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n    \n    return bboxes\n\ndef coco2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normolizinig\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n    \n    return bboxes\n\ndef yolo2coco(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    \n    return bboxes\n\ndef voc2coco(bboxes, image_height=720, image_width=1280):\n    bboxes  = voc2yolo(bboxes, image_height, image_width)\n    bboxes  = yolo2coco(bboxes, image_height, image_width)\n    return bboxes\n\n\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n     \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n                h  = round(float(bbox[3])*image.shape[0]/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:01.775752Z","iopub.execute_input":"2022-02-08T16:33:01.776328Z","iopub.status.idle":"2022-02-08T16:33:01.813107Z","shell.execute_reply.started":"2022-02-08T16:33:01.776293Z","shell.execute_reply":"2022-02-08T16:33:01.812316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##############################################################\n#                      Tracking helpers                      #\n##############################################################\n\nimport numpy as np\nfrom norfair import Detection, Tracker\n\n# Helper to convert bbox in format [x_min, y_min, x_max, y_max, score] to norfair.Detection class\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n        w, h = x_max - x_min, y_max - y_min\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n        \n    return result\n\n# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:02.00805Z","iopub.execute_input":"2022-02-08T16:33:02.008659Z","iopub.status.idle":"2022-02-08T16:33:02.061867Z","shell.execute_reply.started":"2022-02-08T16:33:02.008621Z","shell.execute_reply":"2022-02-08T16:33:02.061209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:02.247881Z","iopub.execute_input":"2022-02-08T16:33:02.248441Z","iopub.status.idle":"2022-02-08T16:33:03.636276Z","shell.execute_reply.started":"2022-02-08T16:33:02.248404Z","shell.execute_reply":"2022-02-08T16:33:03.635256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def load_model(ckpt_path, conf=0.28, iou=0.40):\ndef load_model(ckpt_path, conf=0.28, iou=0.40):\n    model = torch.hub.load('/kaggle/input/yolov5-lib-ds',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload=True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou  # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 20  # maximum number of detections per image\n    return model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:03.638273Z","iopub.execute_input":"2022-02-08T16:33:03.638744Z","iopub.status.idle":"2022-02-08T16:33:03.647735Z","shell.execute_reply.started":"2022-02-08T16:33:03.638707Z","shell.execute_reply":"2022-02-08T16:33:03.646936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🔭 Inference","metadata":{}},{"cell_type":"markdown","source":"## Helper","metadata":{}},{"cell_type":"code","source":"def predict(model, img, size=9000, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # custom inference size\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        return bboxes, confs\n    else:\n        return [],[]\n    \ndef format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:03.64925Z","iopub.execute_input":"2022-02-08T16:33:03.650059Z","iopub.status.idle":"2022-02-08T16:33:03.661763Z","shell.execute_reply.started":"2022-02-08T16:33:03.650019Z","shell.execute_reply":"2022-02-08T16:33:03.661033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tracking_function(tracker, frame_id, bboxes, scores):\n    \n    detects = []\n    predictions = []\n    \n    if len(scores)>0:\n        for i in range(len(bboxes)):\n            box = bboxes[i]\n            score = scores[i]\n            x_min = int(box[0])\n            y_min = int(box[1])\n            bbox_width = int(box[2])\n            bbox_height = int(box[3])\n            detects.append([x_min, y_min, x_min+bbox_width, y_min+bbox_height, score])\n            predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n#             print(predictions[:-1])\n    # Update tracks using detects from current frame\n    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n            continue\n        # Add objects that have no detections on current frame to predictions\n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n        score = tobj.last_detection.scores[0]\n\n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:03.999388Z","iopub.execute_input":"2022-02-08T16:33:04.000046Z","iopub.status.idle":"2022-02-08T16:33:04.011106Z","shell.execute_reply.started":"2022-02-08T16:33:04.000004Z","shell.execute_reply":"2022-02-08T16:33:04.008379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on **Train**","metadata":{}},{"cell_type":"code","source":"CKPT_PATH = '../input/yolov5s6/f2_sub2.pt'\nIMG_SIZE  = 6400\nCONF      = 0.30\nIOU       = 0.50\nAUGMENT   = False\nFDA_aug = False","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:04.627212Z","iopub.execute_input":"2022-02-08T16:33:04.627483Z","iopub.status.idle":"2022-02-08T16:33:04.63184Z","shell.execute_reply.started":"2022-02-08T16:33:04.627454Z","shell.execute_reply":"2022-02-08T16:33:04.630703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CLAHE(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    equalized = clahe.apply(gray)\n    return equalized\ndef Gamma_enhancement(image):\n    gamma = 1/0.6\n    R = 255.0\n    return (R * np.power(image.astype(np.uint32)/R, gamma)).astype(np.uint8)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:04.907808Z","iopub.execute_input":"2022-02-08T16:33:04.908512Z","iopub.status.idle":"2022-02-08T16:33:04.915844Z","shell.execute_reply.started":"2022-02-08T16:33:04.908477Z","shell.execute_reply":"2022-02-08T16:33:04.915187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n\nmodel = load_model(CKPT_PATH, conf=CONF, iou=IOU)\nimage_paths = df[df.num_bbox>1].sample(100).image_path.tolist()\nframe_id = 0\nfor idx, path in enumerate(image_paths):\n    img = cv2.imread(path)[...,::-1]\n    if FDA_aug:\n        img = FDA_trans(image=img)['image']\n    bboxes, confis = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    predict_box = tracking_function(tracker, frame_id, bboxes, confis)\n\n    if len(predict_box)>0:\n        box = [list(map(int,box.split(' ')[1:])) for box in predict_box]\n    else:\n        box = []\n    display(show_img(img, box, bbox_format='coco'))\n    if idx>5:\n        break\n    frame_id += 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:05.206981Z","iopub.execute_input":"2022-02-08T16:33:05.207976Z","iopub.status.idle":"2022-02-08T16:33:18.903581Z","shell.execute_reply.started":"2022-02-08T16:33:05.207925Z","shell.execute_reply":"2022-02-08T16:33:18.902803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Init `Env`","metadata":{}},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:18.905742Z","iopub.execute_input":"2022-02-08T16:33:18.906205Z","iopub.status.idle":"2022-02-08T16:33:18.931136Z","shell.execute_reply.started":"2022-02-08T16:33:18.906168Z","shell.execute_reply":"2022-02-08T16:33:18.930399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd ../working","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:18.932479Z","iopub.execute_input":"2022-02-08T16:33:18.932762Z","iopub.status.idle":"2022-02-08T16:33:18.938784Z","shell.execute_reply.started":"2022-02-08T16:33:18.932715Z","shell.execute_reply":"2022-02-08T16:33:18.93797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on **Test**","metadata":{}},{"cell_type":"code","source":"tracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n\nmodel = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n\nframe_id =0\nfor idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n    if FDA_aug:\n        img = FDA_trans(image=img)['image']\n    bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n\n    predictions = tracking_function(tracker, frame_id, bboxes, confs)\n    \n    prediction_str = ' '.join(predictions)\n    pred_df['annotations'] = prediction_str\n    env.predict(pred_df)\n    if frame_id < 3:\n        if len(predict_box)>0:\n            box = [list(map(int,box.split(' ')[1:])) for box in predictions]\n        else:\n            box = []\n        display(show_img(img, box, bbox_format='coco'))\n#     print('Prediction:', pred_df)\n    frame_id += 1\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:18.941179Z","iopub.execute_input":"2022-02-08T16:33:18.941704Z","iopub.status.idle":"2022-02-08T16:33:21.240929Z","shell.execute_reply.started":"2022-02-08T16:33:18.94164Z","shell.execute_reply":"2022-02-08T16:33:21.24004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 👀 Check Submission","metadata":{}},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:21.242638Z","iopub.execute_input":"2022-02-08T16:33:21.243095Z","iopub.status.idle":"2022-02-08T16:33:21.256456Z","shell.execute_reply.started":"2022-02-08T16:33:21.243059Z","shell.execute_reply":"2022-02-08T16:33:21.255652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}